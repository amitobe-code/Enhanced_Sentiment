{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import flair\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "flair_model = flair.models.TextClassifier.load('en-sentiment')\n",
    "\n",
    "def process_flair(dialogue):   # returning the flair score\n",
    "    sentence = flair.data.Sentence(dialogue)\n",
    "    flair_model.predict(sentence)\n",
    "    label = sentence.labels[0].value\n",
    "    score = sentence.labels[0].score\n",
    "    if label == 'POSITIVE':\n",
    "        return score\n",
    "    elif label == 'NEGATIVE':\n",
    "        return -score\n",
    "\n",
    "\n",
    "\n",
    "def return_sentiment(txt):  # returning single-sentence BERT score\n",
    "    encoded_input = tokenizer(txt, return_tensors='pt',padding=True,truncation=True)\n",
    "    output = bert_model(**encoded_input)\n",
    "    score = output[0][0].detach().numpy() \n",
    "    scores = softmax(score)\n",
    "    if np.argsort(scores)[2] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return (np.argsort(scores)[2]-1)*scores[np.argsort(scores)[2]]\n",
    "    \n",
    "def tb_score(txt):\n",
    "    sen = TextBlob(txt)\n",
    "    return pd.Series({'tb': sen.sentiment.polarity})\n",
    "\n",
    "def cal_vader_textblob_bert_flair(txt):\n",
    "    tb_score = TextBlob(txt).sentiment.polarity\n",
    "    obj = SentimentIntensityAnalyzer()\n",
    "    vader_score = obj.polarity_scores(txt)['compound']\n",
    "    flair_score = process_flair(txt)\n",
    "    bert_score = return_sentiment(txt)\n",
    "    #prob = logmodel3.predict_proba([[tb_score,vader_score,flair_score,bert_score]])[0]      \n",
    "    return np.array([[vader_score, tb_score,bert_score,flair_score]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_tfmr = FunctionTransformer(func=cal_vader_textblob_bert_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the wrapper\n",
    "class PredictionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_func(arr):\n",
    "    negative = (arr[0][0]+arr[0][2])/2\n",
    "    positive = (arr[0][1]+arr[0][3])/2\n",
    "    #if positive >= negative:\n",
    "    #    return 1\n",
    "    #else:\n",
    "    #    return -1\n",
    "    if arr[0][1]>=arr[0][0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = joblib.load('final_pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input as a sentence from the user\n",
    "input_sentence = input(\"Enter a sentence: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = final_pipeline.transform(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered:\" This is a good ice cream \"  .....Sentiment is: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the input sentence\n",
    "print(\"You entered:\\\"\",input_sentence, \"\\\"  .....Sentiment is:\", senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
