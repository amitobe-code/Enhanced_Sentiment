{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining for Stacked Ensemble Sentiment Analysis Model\n",
    "# Fused Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages (from nltk) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "from sklearn import set_config\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "flair_model = flair.models.TextClassifier.load('en-sentiment')\n",
    "\n",
    "def process_flair(dialogue):   # returning the flair score\n",
    "    sentence = flair.data.Sentence(dialogue)\n",
    "    flair_model.predict(sentence)\n",
    "    label = sentence.labels[0].value\n",
    "    score = sentence.labels[0].score\n",
    "    if label == 'POSITIVE':\n",
    "        return score\n",
    "    elif label == 'NEGATIVE':\n",
    "        return -score\n",
    "\n",
    "\n",
    "\n",
    "def return_sentiment(txt):  # returning single-sentence BERT score\n",
    "    encoded_input = tokenizer(txt, return_tensors='pt',padding=True,truncation=True)\n",
    "    output = bert_model(**encoded_input)\n",
    "    score = output[0][0].detach().numpy() \n",
    "    scores = softmax(score)\n",
    "    if np.argsort(scores)[2] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return (np.argsort(scores)[2]-1)*scores[np.argsort(scores)[2]]\n",
    "    \n",
    "def tb_score(txt):\n",
    "    sen = TextBlob(txt)\n",
    "    return pd.Series({'tb': sen.sentiment.polarity})\n",
    "\n",
    "def cal_vader_textblob_bert_flair(txt):\n",
    "    tb_score = TextBlob(txt).sentiment.polarity\n",
    "    obj = SentimentIntensityAnalyzer()\n",
    "    vader_score = obj.polarity_scores(txt)['compound']\n",
    "    flair_score = process_flair(txt)\n",
    "    bert_score = return_sentiment(txt)\n",
    "    #prob = logmodel3.predict_proba([[tb_score,vader_score,flair_score,bert_score]])[0]      \n",
    "    return np.array([[vader_score, tb_score,bert_score,flair_score]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (replace 'trained_model.pkl' with your model file)\n",
    "fusedersational_gb_loaded_model = joblib.load('fused_gb_classifier.pkl')\n",
    "fusedersational_lr_loaded_model = joblib.load('fused_logmodel.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusedersational_lr_loaded_model.predict([[1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_tfmr = FunctionTransformer(func=cal_vader_textblob_bert_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6369    , 0.5       , 0.97106189, 0.99931455]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_tfmr.transform(\"I love this pastry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_LR_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',fusedersational_lr_loaded_model)\n",
    "])\n",
    "fused_GB_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',fusedersational_gb_loaded_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_LR_Pipe.predict(\"I am so happy for the airline that my baggage was delayed by over 40 hours\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Parallel Pipelines\n",
    "Here we make use of a wrapper to enable Fit Transform since our model is only predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the wrapper\n",
    "class PredictionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_LR_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',PredictionTransformer(model =fusedersational_lr_loaded_model ))\n",
    "])\n",
    "fused_GB_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_gb',PredictionTransformer(model =fusedersational_gb_loaded_model ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05317613, 0.94682387]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_LR_Pipe.transform(\"This is a tasty ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1715332, 0.8284668]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_GB_Pipe.transform(\"This is a tasty ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_parallel_pipe = FeatureUnion([\n",
    "    ('pipe_fused_lr', fused_LR_Pipe),\n",
    "    ('pipe_fused_gb',fused_GB_Pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display = \"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 {color: black;background-color: white;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 pre{padding: 0;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-toggleable {background-color: white;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-estimator:hover {background-color: #d4ebff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-item {z-index: 1;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-parallel-item:only-child::after {width: 0;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-c38997d1-f1e7-47fc-9ae5-3f01b11ac864\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_fused_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_fused_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        n_estimators=150,\n",
       "                                                                                                        random_state=42)))]))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"11645211-21b4-4e37-b664-f28d2c6b5e80\" type=\"checkbox\" ><label for=\"11645211-21b4-4e37-b664-f28d2c6b5e80\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_fused_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_fused_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        n_estimators=150,\n",
       "                                                                                                        random_state=42)))]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_fused_lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"51695237-995f-4a8e-bce6-9e7e1e247f2c\" type=\"checkbox\" ><label for=\"51695237-995f-4a8e-bce6-9e7e1e247f2c\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5122ab94-b519-4dad-9172-672d7dd70be9\" type=\"checkbox\" ><label for=\"5122ab94-b519-4dad-9172-672d7dd70be9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_lr: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"81e78544-0645-4cb1-8e0c-1ac9c3b5835d\" type=\"checkbox\" ><label for=\"81e78544-0645-4cb1-8e0c-1ac9c3b5835d\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_fused_gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cacde579-54b7-4a43-bcd3-1c46c9177e03\" type=\"checkbox\" ><label for=\"cacde579-54b7-4a43-bcd3-1c46c9177e03\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"23ef3aff-43cf-4616-ad7c-ba9e5f335235\" type=\"checkbox\" ><label for=\"23ef3aff-43cf-4616-ad7c-ba9e5f335235\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_gb: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                       n_estimators=150,\n",
       "                                                       random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2e07124b-8e13-4a91-9943-cb2cf662032c\" type=\"checkbox\" ><label for=\"2e07124b-8e13-4a91-9943-cb2cf662032c\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=150,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "FeatureUnion(transformer_list=[('pipe_fused_lr',\n",
       "                                Pipeline(steps=[('base_models_scores',\n",
       "                                                 FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7ff01a7f4290>)),\n",
       "                                                ('meta_model_lr',\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               ('pipe_fused_gb',\n",
       "                                Pipeline(steps=[('base_models_scores',\n",
       "                                                 FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7ff01a7f4290>)),\n",
       "                                                ('meta_model_gb',\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        n_estimators=150,\n",
       "                                                                                                        random_state=42)))]))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_parallel_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00017665754538126997"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_parallel_pipe.transform(\"This is not a tasty ice cream\")[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_func(arr):\n",
    "    negative = (arr[0][0]+arr[0][2])/2\n",
    "    positive = (arr[0][1]+arr[0][3])/2\n",
    "    #if positive >= negative:\n",
    "    #    return 1\n",
    "    #else:\n",
    "    #    return -1\n",
    "    if arr[0][1]>=arr[0][0]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_final_func = FunctionTransformer(func=fused_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_final_pipeline = Pipeline([\n",
    "    ('fused_overall_pipe',fused_parallel_pipe),\n",
    "    ('fused_final_func', fused_final_func)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_final_pipeline.transform(\"This is not a tasty ice cream, this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 {color: black;background-color: white;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 pre{padding: 0;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-toggleable {background-color: white;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-item {z-index: 1;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-parallel-item:only-child::after {width: 0;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-4e7936ef-9332-453a-9e7e-2e0e229b6fe3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;fused_overall_pipe&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;pipe_fused_lr&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                                 (&#x27;meta_model_lr&#x27;,\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                (&#x27;pipe_fused_gb&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                                 (&#x27;meta_model_gb&#x27;,\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         n_estimators=150,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                (&#x27;fused_final_func&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function fused_func at 0x7ff019b75170&gt;))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"73ed21f8-66f9-4321-9a26-a9f02e7fdbc5\" type=\"checkbox\" ><label for=\"73ed21f8-66f9-4321-9a26-a9f02e7fdbc5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;fused_overall_pipe&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;pipe_fused_lr&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                                 (&#x27;meta_model_lr&#x27;,\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                (&#x27;pipe_fused_gb&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                                 (&#x27;meta_model_gb&#x27;,\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         n_estimators=150,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                (&#x27;fused_final_func&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function fused_func at 0x7ff019b75170&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1e158a10-08be-43e5-a157-3b62981b483d\" type=\"checkbox\" ><label for=\"1e158a10-08be-43e5-a157-3b62981b483d\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">fused_overall_pipe: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_fused_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_fused_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        n_estimators=150,\n",
       "                                                                                                        random_state=42)))]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_fused_lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0fc64acb-fb8b-413d-8499-ea6b126174e2\" type=\"checkbox\" ><label for=\"0fc64acb-fb8b-413d-8499-ea6b126174e2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"da8dac2d-980e-4bf6-8579-b3a6e1574eed\" type=\"checkbox\" ><label for=\"da8dac2d-980e-4bf6-8579-b3a6e1574eed\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_lr: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a76b0336-c866-4fc8-b123-765729c10993\" type=\"checkbox\" ><label for=\"a76b0336-c866-4fc8-b123-765729c10993\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_fused_gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e188d452-5083-4151-954a-69abc14bcd73\" type=\"checkbox\" ><label for=\"e188d452-5083-4151-954a-69abc14bcd73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7ff01a7f4290&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2d9b784e-df3d-48cd-b124-24bdea4046c9\" type=\"checkbox\" ><label for=\"2d9b784e-df3d-48cd-b124-24bdea4046c9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_gb: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                       n_estimators=150,\n",
       "                                                       random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"984ce484-fc2f-4ad1-beb0-3a5ddee24768\" type=\"checkbox\" ><label for=\"984ce484-fc2f-4ad1-beb0-3a5ddee24768\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=150,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"261fce6b-34a2-4aec-8522-0d2d231ed1c2\" type=\"checkbox\" ><label for=\"261fce6b-34a2-4aec-8522-0d2d231ed1c2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function fused_func at 0x7ff019b75170&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('fused_overall_pipe',\n",
       "                 FeatureUnion(transformer_list=[('pipe_fused_lr',\n",
       "                                                 Pipeline(steps=[('base_models_scores',\n",
       "                                                                  FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7ff01a7f4290>)),\n",
       "                                                                 ('meta_model_lr',\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                ('pipe_fused_gb',\n",
       "                                                 Pipeline(steps=[('base_models_scores',\n",
       "                                                                  FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7ff01a7f4290>)),\n",
       "                                                                 ('meta_model_gb',\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         n_estimators=150,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                ('fused_final_func',\n",
       "                 FunctionTransformer(func=<function fused_func at 0x7ff019b75170>))])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_config(display = \"diagram\")\n",
    "fused_final_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_testdf = pd.read_csv('sst2_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1000\n",
       "-1    1000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_testdf['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>vader</th>\n",
       "      <th>textblob</th>\n",
       "      <th>bert</th>\n",
       "      <th>flair</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>bert_polarity</th>\n",
       "      <th>flair_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>to see a movie with its heart</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>so bad it does n't improve upon the experience...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.7351</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.927613</td>\n",
       "      <td>-0.999958</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the reason to go see `` blue crush '' is the p...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.962698</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>impostor is a step down for director gary fled...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999793</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>satisfyingly scarifying</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.672349</td>\n",
       "      <td>0.998684</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>an undeniably moving film to experience ,</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.971453</td>\n",
       "      <td>0.999741</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>the compelling historical tale</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.791588</td>\n",
       "      <td>0.999016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>the film buzz and whir ; very little of it</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.243750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.997583</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>, runteldat is something of a triumph .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940723</td>\n",
       "      <td>0.998238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>death to smoochy is often very funny , but wha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.584771</td>\n",
       "      <td>0.999843</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Headline  \\\n",
       "0              0                     to see a movie with its heart    \n",
       "1              1  so bad it does n't improve upon the experience...   \n",
       "2              2  the reason to go see `` blue crush '' is the p...   \n",
       "3              3  impostor is a step down for director gary fled...   \n",
       "4              4                           satisfyingly scarifying    \n",
       "...          ...                                                ...   \n",
       "1995        1995         an undeniably moving film to experience ,    \n",
       "1996        1996                    the compelling historical tale    \n",
       "1997        1997        the film buzz and whir ; very little of it    \n",
       "1998        1998           , runteldat is something of a triumph .    \n",
       "1999        1999  death to smoochy is often very funny , but wha...   \n",
       "\n",
       "      Sentiment   vader  textblob      bert     flair  vader_polarity  \\\n",
       "0             1  0.0000  0.000000  0.000000  0.996216               1   \n",
       "1            -1 -0.7351 -0.350000 -0.927613 -0.999958              -1   \n",
       "2             1 -0.1531  0.250000  0.962698  0.999964              -1   \n",
       "3            -1  0.0000 -0.155556  0.000000 -0.999793               1   \n",
       "4             1  0.4404  0.500000 -0.672349  0.998684               1   \n",
       "...         ...     ...       ...       ...       ...             ...   \n",
       "1995          1  0.0000  0.000000  0.971453  0.999741               1   \n",
       "1996          1  0.2263  0.150000  0.791588  0.999016               1   \n",
       "1997         -1  0.0000 -0.243750  0.000000 -0.997583               1   \n",
       "1998          1  0.4767  0.000000  0.940723  0.998238               1   \n",
       "1999          1  0.8976  0.525000  0.584771  0.999843               1   \n",
       "\n",
       "      textblob_polarity  bert_polarity  flair_polarity  \n",
       "0                     1              1               1  \n",
       "1                    -1             -1              -1  \n",
       "2                     1              1               1  \n",
       "3                    -1              1              -1  \n",
       "4                     1             -1               1  \n",
       "...                 ...            ...             ...  \n",
       "1995                  1              1               1  \n",
       "1996                  1              1               1  \n",
       "1997                 -1              1              -1  \n",
       "1998                  1              1               1  \n",
       "1999                  1              1               1  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_senti_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(fused_testdf)):\n",
    "    senti = fused_final_pipeline.transform(fused_testdf['Headline'].iloc[i])\n",
    "    fused_senti_output.append(senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " ...]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_senti_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      1.00      0.80      1000\n",
      "           1       1.00      0.50      0.67      1000\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.83      0.75      0.74      2000\n",
      "weighted avg       0.83      0.75      0.74      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(fused_testdf['Sentiment'],fused_senti_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_pipe.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(fused_final_pipeline,'final_pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
