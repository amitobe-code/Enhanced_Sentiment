{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining for Stacked Ensemble Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages (3.3)\n",
      "Requirement already satisfied: six in /Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages (from nltk) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "from sklearn import set_config\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "flair_model = flair.models.TextClassifier.load('en-sentiment')\n",
    "\n",
    "def process_flair(dialogue):   # returning the flair score\n",
    "    sentence = flair.data.Sentence(dialogue)\n",
    "    flair_model.predict(sentence)\n",
    "    label = sentence.labels[0].value\n",
    "    score = sentence.labels[0].score\n",
    "    if label == 'POSITIVE':\n",
    "        return score\n",
    "    elif label == 'NEGATIVE':\n",
    "        return -score\n",
    "\n",
    "\n",
    "\n",
    "def return_sentiment(txt):  # returning single-sentence BERT score\n",
    "    encoded_input = tokenizer(txt, return_tensors='pt',padding=True,truncation=True)\n",
    "    output = bert_model(**encoded_input)\n",
    "    score = output[0][0].detach().numpy() \n",
    "    scores = softmax(score)\n",
    "    if np.argsort(scores)[2] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return (np.argsort(scores)[2]-1)*scores[np.argsort(scores)[2]]\n",
    "    \n",
    "def tb_score(txt):\n",
    "    sen = TextBlob(txt)\n",
    "    return pd.Series({'tb': sen.sentiment.polarity})\n",
    "\n",
    "def cal_vader_textblob_bert_flair(txt):\n",
    "    tb_score = TextBlob(txt).sentiment.polarity\n",
    "    obj = SentimentIntensityAnalyzer()\n",
    "    vader_score = obj.polarity_scores(txt)['compound']\n",
    "    flair_score = process_flair(txt)\n",
    "    bert_score = return_sentiment(txt)\n",
    "    #prob = logmodel3.predict_proba([[tb_score,vader_score,flair_score,bert_score]])[0]      \n",
    "    return np.array([[vader_score, tb_score,bert_score,flair_score]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model (replace 'trained_model.pkl' with your model file)\n",
    "conversational_gb_loaded_model = joblib.load('conversational_gb_sav_model.pkl')\n",
    "conversational_lr_loaded_model = joblib.load('conversational_lr_sav_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_lr_loaded_model.predict([[1,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_tfmr = FunctionTransformer(func=cal_vader_textblob_bert_flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6369    , 0.5       , 0.97106189, 0.99931455]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_tfmr.transform(\"I love this pastry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv_LR_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',conversational_lr_loaded_model)\n",
    "])\n",
    "Conv_GB_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',conversational_gb_loaded_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv_LR_Pipe.predict(\"I just very much dislike hate kill this pastry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Parallel Pipelines\n",
    "Here we make use of a wrapper to enable Fit Transform since our model is only predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the wrapper\n",
    "class PredictionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv_LR_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_lr',PredictionTransformer(model =conversational_lr_loaded_model ))\n",
    "])\n",
    "Conv_GB_Pipe = Pipeline ([\n",
    "    ('base_models_scores',func_tfmr),\n",
    "    ('meta_model_gb',PredictionTransformer(model =conversational_gb_loaded_model ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04317196, 0.95682804]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv_LR_Pipe.transform(\"This is a tasty ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24905813, 0.75094187]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv_GB_Pipe.transform(\"This is a tasty ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_parallel_pipe = FeatureUnion([\n",
    "    ('pipe_conv_lr', Conv_LR_Pipe),\n",
    "    ('pipe_conv_gb',Conv_GB_Pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display = \"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 {color: black;background-color: white;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 pre{padding: 0;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-toggleable {background-color: white;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-item {z-index: 1;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-parallel-item:only-child::after {width: 0;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-078b6cce-6375-4042-b7a4-e95a76c6bca9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-078b6cce-6375-4042-b7a4-e95a76c6bca9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_conv_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_conv_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        random_state=42)))]))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"35b71783-2956-4922-9a16-c657561910b4\" type=\"checkbox\" ><label for=\"35b71783-2956-4922-9a16-c657561910b4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_conv_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_conv_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        random_state=42)))]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_conv_lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8751f2c5-84ec-422f-8489-87d8457ed802\" type=\"checkbox\" ><label for=\"8751f2c5-84ec-422f-8489-87d8457ed802\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ce464dfb-3541-4a46-adcd-2111ef24ea64\" type=\"checkbox\" ><label for=\"ce464dfb-3541-4a46-adcd-2111ef24ea64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_lr: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c413bea9-181f-4dc6-81b7-25aac8b3c48d\" type=\"checkbox\" ><label for=\"c413bea9-181f-4dc6-81b7-25aac8b3c48d\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_conv_gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bb353e29-0e98-4741-bab7-09985c4f5ff6\" type=\"checkbox\" ><label for=\"bb353e29-0e98-4741-bab7-09985c4f5ff6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d038413d-973d-4146-acac-8bc648251799\" type=\"checkbox\" ><label for=\"d038413d-973d-4146-acac-8bc648251799\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_gb: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                       random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0e8012f7-7117-4a16-b9e7-e3115a4848fa\" type=\"checkbox\" ><label for=\"0e8012f7-7117-4a16-b9e7-e3115a4848fa\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "FeatureUnion(transformer_list=[('pipe_conv_lr',\n",
       "                                Pipeline(steps=[('base_models_scores',\n",
       "                                                 FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7f8e08e39e60>)),\n",
       "                                                ('meta_model_lr',\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               ('pipe_conv_gb',\n",
       "                                Pipeline(steps=[('base_models_scores',\n",
       "                                                 FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7f8e08e39e60>)),\n",
       "                                                ('meta_model_gb',\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        random_state=42)))]))])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_parallel_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_parallel_pipe.transform(\"This is a tasty ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_func(arr):\n",
    "    negative = (arr[0][0]+arr[0][2])/2\n",
    "    positive = (arr[0][1]+arr[0][3])/2\n",
    "    #if positive >= negative:\n",
    "    #    return 1\n",
    "    #else:\n",
    "    #    return -1\n",
    "    if arr[0][1]>=arr[0][1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_final_func = FunctionTransformer(func=conv_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv_final_pipeline = Pipeline([\n",
    "    ('conv_overall_pipe',conv_parallel_pipe),\n",
    "    ('conv_final_func', conv_final_func)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv_final_pipeline.transform(\"should I be considering so many deaths today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 {color: black;background-color: white;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 pre{padding: 0;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-toggleable {background-color: white;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-estimator:hover {background-color: #d4ebff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-item {z-index: 1;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-parallel-item:only-child::after {width: 0;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-b11885e6-8c8f-4ef6-8c2b-adef3f0d9491\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;conv_overall_pipe&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;pipe_conv_lr&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                                 (&#x27;meta_model_lr&#x27;,\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                (&#x27;pipe_conv_gb&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                                 (&#x27;meta_model_gb&#x27;,\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                (&#x27;conv_final_func&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function conv_func at 0x7f8dda6d2a70&gt;))])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b1a2b621-4ce4-4961-8980-386104f221d6\" type=\"checkbox\" ><label for=\"b1a2b621-4ce4-4961-8980-386104f221d6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;conv_overall_pipe&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;pipe_conv_lr&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                                 (&#x27;meta_model_lr&#x27;,\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                (&#x27;pipe_conv_gb&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                                  FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                                 (&#x27;meta_model_gb&#x27;,\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                (&#x27;conv_final_func&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function conv_func at 0x7f8dda6d2a70&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cceaaefc-bb4c-4227-a34a-1f1f745fc2f8\" type=\"checkbox\" ><label for=\"cceaaefc-bb4c-4227-a34a-1f1f745fc2f8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">conv_overall_pipe: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;pipe_conv_lr&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_lr&#x27;,\n",
       "                                                 PredictionTransformer(model=LogisticRegression()))])),\n",
       "                               (&#x27;pipe_conv_gb&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;base_models_scores&#x27;,\n",
       "                                                 FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)),\n",
       "                                                (&#x27;meta_model_gb&#x27;,\n",
       "                                                 PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                        random_state=42)))]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_conv_lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"37a01dda-34c9-4deb-b4df-2006997877f0\" type=\"checkbox\" ><label for=\"37a01dda-34c9-4deb-b4df-2006997877f0\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7b2100f8-92e7-4b8f-af6f-0e729d918ccc\" type=\"checkbox\" ><label for=\"7b2100f8-92e7-4b8f-af6f-0e729d918ccc\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_lr: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b83c9841-6ecf-4cf4-b616-a445571f3d99\" type=\"checkbox\" ><label for=\"b83c9841-6ecf-4cf4-b616-a445571f3d99\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>pipe_conv_gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e3884f72-ecfd-420c-85f8-f04b5146d22f\" type=\"checkbox\" ><label for=\"e3884f72-ecfd-420c-85f8-f04b5146d22f\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function cal_vader_textblob_bert_flair at 0x7f8e08e39e60&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8d8594b3-24e6-4c39-800f-9edaa295b7ad\" type=\"checkbox\" ><label for=\"8d8594b3-24e6-4c39-800f-9edaa295b7ad\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">meta_model_gb: PredictionTransformer</label><div class=\"sk-toggleable__content\"><pre>PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                       random_state=42))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a90df1ef-1b17-4e09-ac18-92f34776dab2\" type=\"checkbox\" ><label for=\"a90df1ef-1b17-4e09-ac18-92f34776dab2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.01, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c851dbca-afea-4cf1-8712-817d59abd7ef\" type=\"checkbox\" ><label for=\"c851dbca-afea-4cf1-8712-817d59abd7ef\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function conv_func at 0x7f8dda6d2a70&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('conv_overall_pipe',\n",
       "                 FeatureUnion(transformer_list=[('pipe_conv_lr',\n",
       "                                                 Pipeline(steps=[('base_models_scores',\n",
       "                                                                  FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7f8e08e39e60>)),\n",
       "                                                                 ('meta_model_lr',\n",
       "                                                                  PredictionTransformer(model=LogisticRegression()))])),\n",
       "                                                ('pipe_conv_gb',\n",
       "                                                 Pipeline(steps=[('base_models_scores',\n",
       "                                                                  FunctionTransformer(func=<function cal_vader_textblob_bert_flair at 0x7f8e08e39e60>)),\n",
       "                                                                 ('meta_model_gb',\n",
       "                                                                  PredictionTransformer(model=GradientBoostingClassifier(learning_rate=0.01,\n",
       "                                                                                                                         random_state=42)))]))])),\n",
       "                ('conv_final_func',\n",
       "                 FunctionTransformer(func=<function conv_func at 0x7f8dda6d2a70>))])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_config(display = \"diagram\")\n",
    "Conv_final_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_testdf = pd.read_csv('conversational_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1000\n",
       "-1    1000\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_testdf['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50368</td>\n",
       "      <td>Well , the life style is much more relaxed th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>75655</td>\n",
       "      <td>I have a complaint to make . I've just been ba...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>43391</td>\n",
       "      <td>Yes , orange juice will be fine for me . But ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>18291</td>\n",
       "      <td>Your best isn't good enough . Back in my day ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>42665</td>\n",
       "      <td>Happy Women's Day . I love you forever , Mum .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>6702</td>\n",
       "      <td>Thank you .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>44170</td>\n",
       "      <td>Oh , Mary , come in , please . I'm so happy to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>37295</td>\n",
       "      <td>I hate to see the abuse of animals .</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>12770</td>\n",
       "      <td>Happy birthday , John . Many happy returns of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>68403</td>\n",
       "      <td>Good-bye . See you again sometime .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  \\\n",
       "0              0         50368   \n",
       "1              1         75655   \n",
       "2              2         43391   \n",
       "3              3         18291   \n",
       "4              4         42665   \n",
       "...          ...           ...   \n",
       "1995        1995          6702   \n",
       "1996        1996         44170   \n",
       "1997        1997         37295   \n",
       "1998        1998         12770   \n",
       "1999        1999         68403   \n",
       "\n",
       "                                               Headline  Sentiment  \n",
       "0      Well , the life style is much more relaxed th...          1  \n",
       "1     I have a complaint to make . I've just been ba...         -1  \n",
       "2      Yes , orange juice will be fine for me . But ...          1  \n",
       "3      Your best isn't good enough . Back in my day ...         -1  \n",
       "4       Happy Women's Day . I love you forever , Mum .           1  \n",
       "...                                                 ...        ...  \n",
       "1995                                       Thank you .           1  \n",
       "1996  Oh , Mary , come in , please . I'm so happy to...          1  \n",
       "1997              I hate to see the abuse of animals .          -1  \n",
       "1998  Happy birthday , John . Many happy returns of ...          1  \n",
       "1999               Good-bye . See you again sometime .           1  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_senti_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(conv_testdf)):\n",
    "    senti = Conv_final_pipeline.transform(conv_testdf['Headline'].iloc[i])\n",
    "    conv_senti_output.append(senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_senti_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00      1000\n",
      "           1       0.50      1.00      0.67      1000\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.25      0.50      0.33      2000\n",
      "weighted avg       0.25      0.50      0.33      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/amitobe/opt/anaconda3/envs/nlp_course/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(conv_testdf['Sentiment'],conv_senti_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
